{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutual Fund Investment Prediction\n",
    "## Using RNN (Long Short Term Memory Prediction)\n",
    "\n",
    "LSTM has a special architecture which enables it to forget the unnecessary information. \n",
    "\n",
    "1. The sigmoid layer takes the input X(t) and h(t-1) and decides which parts from old output should be removed (by outputting a 0).\n",
    "2. The next step is to decide and store information from the new input X(t) in the cell state. A Sigmoid layer decides which of the new information should be updated or ignored. A tanh layer creates a vector of all the possible values from the new input. These two are multiplied to update the new cell sate. This new memory is then added to old memory c(t-1) to give c(t).\n",
    "3. Finally, we need to decide what we’re going to output. A sigmoid layer decides which parts of the cell state we are going to output. Then, we put the cell state through a tanh generating all the possible values and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to. Our model does not learn this answer from the immediate dependency, rather it learnt it from long term dependency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing all the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import pandas as pd\n",
    "from datetime import datetime,timedelta,date\n",
    "import smtplib\n",
    "from email.message import EmailMessage\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to Google Sheet\n",
    "\n",
    "Our primary data source is google sheet, where the daily changes for all the mutual funds, are recorded.\n",
    "\n",
    "To connect the Google Sheet, we perform the following -\n",
    "\n",
    "1. Go to https://console.cloud.google.com/ and create a new Project.\n",
    "2. In the created project, enable Google Drive API\n",
    "3. Create credentials to access the Google Drive API.\n",
    "4. Enable the Google Sheets API\n",
    "5. Share the Google Sheet with the dev ID generated in the credential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the scope of the OAuth Authentication\n",
    "scope = [\"https://spreadsheets.google.com/feeds\",'https://www.googleapis.com/auth/spreadsheets',\"https://www.googleapis.com/auth/drive.file\",\"https://www.googleapis.com/auth/drive\"]\n",
    "         \n",
    "#Getting the credentials\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name(\"D:/Codebase/Mutual_Fund_Analysis/Backend/mutual-fund-analysis/secret_config/google_credentials.json\", scope)\n",
    "#Connecting to the Google Spreadsheet Client\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "#Getting the spreadsheet\n",
    "sheet = client.open(\"Daily_MF_Returns\").sheet1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_data_list = sheet.get_all_records()\n",
    "\n",
    "#Creating the dataframe\n",
    "mf_data = pd.DataFrame(mf_data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Conversion\n",
    "### Convert the Data in Date and Value format with GROUP BY DATE\n",
    "\n",
    "The following changes needs to be done on the data - \n",
    "1. Group the data by dates and calculate sum.\n",
    "2. Drop all other columns except the date and sum column\n",
    "3. Create a new Date Column in YYYY-MM-DD format\n",
    "4. Drop the previous date column\n",
    "5. Rename the columns => Date, Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Date     Value\n",
      "38  2020-11-19   6676.27\n",
      "39  2020-11-20   6670.78\n",
      "40  2020-11-21   6686.48\n",
      "41  2020-11-24   6697.89\n",
      "42  2020-11-25   6685.71\n",
      "..         ...       ...\n",
      "33  2021-02-03  14848.78\n",
      "34  2021-02-04  14903.25\n",
      "35  2021-02-05  14923.36\n",
      "36  2021-02-06  14886.40\n",
      "37  2021-02-07  14886.40\n",
      "\n",
      "[71 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "grouped_mf_data = mf_data.groupby('Date',as_index=False).agg({'Return':[np.sum]})\n",
    "#Renaming the columns\n",
    "grouped_mf_data.columns = ['Date','Value']\n",
    "\n",
    "#Create a new date column to sort the data\n",
    "grouped_mf_data['Modified_Date']=grouped_mf_data['Date'].apply(lambda x : datetime.strptime(x, '%m/%d/%Y'))\n",
    "grouped_mf_data.sort_values(by=['Modified_Date'], inplace=True, ascending=True)\n",
    "\n",
    "#Create the sting equivalent of the date olum\n",
    "grouped_mf_data['Date_Modified_Str']=grouped_mf_data['Date'].apply(lambda x : datetime.strptime(x, '%m/%d/%Y').strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "#Drop the extra columns and rename\n",
    "grouped_mf_data = grouped_mf_data.drop(['Date','Modified_Date'],axis=1)\n",
    "grouped_mf_data.columns = ['Value','Date']\n",
    "grouped_mf_data = grouped_mf_data[['Date','Value']]\n",
    "\n",
    "#Function to determine the color\n",
    "\n",
    "def get_line_color(values):\n",
    "    if(values[-1][1]>values[-2][1]):\n",
    "        return 'green'\n",
    "    elif(values[-1][1]==values[-2][1]):\n",
    "        return 'yellow'\n",
    "    return 'red'\n",
    "\n",
    "#Show graph - Method 2\n",
    "fig, ax = pyplot.subplots()\n",
    "grouped_mf_data['Actual_Date']=grouped_mf_data['Date'].apply(lambda x : datetime.strptime(x, '%Y-%m-%d'))\n",
    "ax.plot(grouped_mf_data['Actual_Date'],grouped_mf_data['Value'], color=get_line_color(grouped_mf_data.values.tolist()))\n",
    "ax.xaxis_date()     # interpret the x-axis values as dates\n",
    "fig.set_size_inches(14, 8, forward=True)\n",
    "fig.autofmt_xdate() # make space for and rotate the x-axis tick labels\n",
    "pyplot.show()\n",
    "\n",
    "grouped_mf_data = grouped_mf_data.drop(['Actual_Date'],axis=1)\n",
    "\n",
    "print(grouped_mf_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the Data Set to make it suitale for the LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping the date column for Time Series Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_mf_data = grouped_mf_data.drop(['Date'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Time Series to Stationary\n",
    "\n",
    "The given dataset is not stationary.\n",
    "\n",
    "This means that there is a structure in the data that is dependent on the time. Specifically, there is an modulating (changing) trend in the data.\n",
    "\n",
    "Stationary data is easier to model and will very likely result in more skillful forecasts.\n",
    "\n",
    "The trend can be removed from the observations, then added back to forecasts later to return the prediction to the original scale and calculate a comparable error score.\n",
    "\n",
    "A standard way to remove a trend is by **differencing the data**. That is the **observation from the previous time step (t-1) is subtracted from the current observation (t).** This removes the trend and we are left with a difference series, or the changes to the observations from one time step to the next.\n",
    "\n",
    "We can achieve this automatically **using the diff() function in pandas.**\n",
    "Alternatively, we can get finer grained control and write our own function to do this, which is preferred for its flexibility in this case.\n",
    "\n",
    "Below is a function called difference() that calculates a differenced series. Note that the first observation in the series is skipped as there is no prior observation with which to calculate a differenced value.\n",
    "\n",
    "We also need to **invert this process in order to take forecasts made on the differenced series back into their original scale.**\n",
    "\n",
    "The function below, called **inverse_difference(), inverts this operation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a differenced series\n",
    "def difference(supervised_converted_data, interval=1):\n",
    "    data_diiference = list()\n",
    "    for i in range(interval, len(supervised_converted_data)):\n",
    "        value = supervised_converted_data[i][0] - supervised_converted_data[i - interval][0]\n",
    "        data_diiference.append(value)\n",
    "    return pd.Series(data_diiference)\n",
    "\n",
    "\n",
    "# invert differenced value\n",
    "def inverse_difference(history, yhat, interval=1):\n",
    "    return yhat + history[-interval]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the data to a supervised learning problem\n",
    "\n",
    "The LSTM model in Keras assumes that your data is divided into input (X) and output (y) components.\n",
    "\n",
    "For a time series problem, we can achieve this by using the observation from the last time step (t-1) as the input and the observation at the current time step (t) as the output.\n",
    "\n",
    "We can achieve this using the shift() function in Pandas that will push all values in a series down by a specified number places. We require a shift of 1 place, which will become the input variables. The time series as it stands will be the output variables.\n",
    "\n",
    "We can then concatenate these two series together to create a DataFrame ready for supervised learning. The pushed-down series will have a new position at the top with no value. A NaN (not a number) value will be used in this position. We will replace these NaN values with 0 values, which the LSTM model will have to learn as “the start of the series” or “I have no data here”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeseries_to_supervised(grouped_data_values, lag=1):\n",
    "    grouped_data = pd.DataFrame(grouped_data_values)\n",
    "    columns = [grouped_data.shift(i) for i in range(1, lag+1)]\n",
    "    columns.append(grouped_data)\n",
    "    supervised_converted_data = pd.concat(columns, axis=1)\n",
    "    supervised_converted_data.fillna(0, inplace=True)\n",
    "    return supervised_converted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into train and test\n",
    "\n",
    "Cannot use test_train_split from sckit-learn for the following reasons -\n",
    "\n",
    "1. The input is list of values (1-D) and not dataset\n",
    "2. There is no X and Y parameters based on this kind of classification\n",
    "3. Only values selected based on differential data value (1-D)\n",
    "\n",
    "The solution is -\n",
    "\n",
    "1. The ratio of split is 70-30\n",
    "2. Based on the ratio split the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percentage(percent, whole):\n",
    "  return (percent * whole) // 100\n",
    "\n",
    "# Splitting the data into test and train for calculating accuracy\n",
    "def get_train_test_data(list_of_values, percentage_of_split=70):\n",
    "    seperator_value = get_percentage(percentage_of_split,len(list_of_values))\n",
    "    \n",
    "    #Since the supervised values will already be one less than the len ngth of original series, we need to consider last n+1\n",
    "    #Instead of the last n values in test data\n",
    "    seperator_value = seperator_value - 1\n",
    "    return list_of_values[:seperator_value],list_of_values[seperator_value:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Time Series to Scale\n",
    "\n",
    "Like other neural networks, **LSTMs expect data to be within the scale of the activation function used by the network.**\n",
    "\n",
    "The default activation function for LSTMs is the **hyperbolic tangent (tanh), which outputs values between -1 and 1.** This is the preferred range for the time series data.\n",
    "\n",
    "To make the experiment fair, the scaling coefficients (min and max) values must be calculated on the training dataset and applied to scale the test dataset and any forecasts. This is to avoid contaminating the experiment with knowledge from the test dataset, which might give the model a small edge.\n",
    "\n",
    "We can transform the dataset to the range [-1, 1] using the MinMaxScaler class. Like other scikit-learn transform classes, it requires data provided in a matrix format with rows and columns. Therefore, we must reshape our NumPy arrays before transforming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale train and test data to [-1, 1]\n",
    "def scale(train, test):\n",
    "    # fit scaler\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler = scaler.fit(train)\n",
    "    # transform train\n",
    "    train = train.reshape(train.shape[0], train.shape[1])\n",
    "    train_scaled = scaler.transform(train)\n",
    "    # transform test\n",
    "    test = test.reshape(test.shape[0], test.shape[1])\n",
    "    test_scaled = scaler.transform(test)\n",
    "    return scaler, train_scaled, test_scaled\n",
    " \n",
    "# inverse scaling for a forecasted value\n",
    "def invert_scale(scaler, X, value):\n",
    "    new_row = [x for x in X] + [value]\n",
    "    array = np.array(new_row)\n",
    "    array = array.reshape(1, len(array))\n",
    "    inverted = scaler.inverse_transform(array)\n",
    "    return inverted[0, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the LTSM Model\n",
    "\n",
    "The shape of the input data must be specified in the LSTM layer using the “batch_input_shape” argument as a tuple that specifies the expected number of observations to read each batch, the number of time steps, and the number of features.\n",
    "\n",
    "The batch size is often much smaller than the total number of samples. It, along with the number of epochs, defines how quickly the network learns the data (how often the weights are updated).\n",
    "\n",
    "The final import parameter in defining the LSTM layer is the number of neurons, also called the number of memory units or blocks. This is a reasonably simple problem and a number between 1 and 5 should be sufficient.\n",
    "\n",
    "The line below creates a single LSTM hidden layer that also specifies the expectations of the input layer via the “batch_input_shape” argument.\n",
    "\n",
    "The network requires a single neuron in the output layer with a linear activation to predict the number of data at the next time step.\n",
    "\n",
    "Once the network is specified, it must be compiled into an efficient symbolic representation using a backend mathematical library, such as TensorFlow or Theano.\n",
    "\n",
    "In compiling the network, we must specify a loss function and optimization algorithm. We will use “mean_squared_error” as the loss function as it closely matches RMSE that we will are interested in, and the efficient ADAM optimization algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit an LSTM network to training data\n",
    "def fit_lstm(train, batch_size, nb_epoch, neurons):\n",
    "    X, y = train[:, 0:-1], train[:, -1]\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    for i in range(nb_epoch):\n",
    "        model.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n",
    "        model.reset_states()\n",
    "    return model\n",
    " \n",
    "# make a one-step forecast\n",
    "def forecast_lstm(model, batch_size, X):\n",
    "    X = X.reshape(1, 1, len(X))\n",
    "    yhat = model.predict(X, batch_size=batch_size)\n",
    "    return yhat[0,0]\n",
    "\n",
    "# make a multi-step forecast\n",
    "def forecast_lstm_multi_step(model, batch_size, X):\n",
    "    X = X.reshape(1, 1, len(X))\n",
    "    yhat = model.predict(X, batch_size=batch_size)\n",
    "    print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplot to compare the best set of epochs and neurons\n",
    "\n",
    "Configuring neural networks is difficult because there is no good theory on how to do it.\n",
    "\n",
    "We must be systematic and explore different configurations both from a dynamical and an objective results point of a view to try to understand what is going on for a given predictive modeling problem.\n",
    "\n",
    "The diagnostic runs above are helpful to explore the dynamical behavior of the model, but fall short of an objective and comparable mean performance.\n",
    "\n",
    "We can address this by repeating the same experiments and calculating and comparing summary statistics for each configuration. In this case, 30 runs were completed of the epoch values 500, 1000, 2000, 4000, and 6000.\n",
    "\n",
    "The idea is to compare the configurations using summary statistics over a larger number of runs and see exactly which of the configurations might perform better on average.\n",
    "\n",
    "#### Variable Definitions\n",
    "1. Epoch - One Epoch is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE. Since one epoch is too big to feed to the computer at once we divide it in several smaller batches.\n",
    "2. Neurons - The number of memory units or blocks. \n",
    "\n",
    "#### Optimization steps\n",
    "1. Set of epochs --> 500, 1000, 2000, 4000, 6000\n",
    "2. Find the optimized numnber of epochs\n",
    "3. Set of neurons --> 1,2,3,4,5\n",
    "4. Find the optimized number of neurons in the most optimized epoch model (evaluated from the preious step)\n",
    "5. Use the most optimized model to retrain finally in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Test RMSE: 155.725\n",
      "2) Test RMSE: 156.053\n",
      "3) Test RMSE: 157.606\n",
      "4) Test RMSE: 154.463\n",
      "1) Test RMSE: 153.325\n",
      "2) Test RMSE: 153.686\n",
      "3) Test RMSE: 153.476\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-834b2a8ccf0d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;31m# epochs = [500]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m     \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepeats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m \u001b[1;31m# summarize results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-50-a007e26d777f>\u001b[0m in \u001b[0;36mexperiment\u001b[1;34m(repeats, epochs)\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;31m# fit the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0mlstm_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_lstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;31m# forecast the entire training dataset to build up state for forecasting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-77-dea469c88dc0>\u001b[0m in \u001b[0;36mfit_lstm\u001b[1;34m(train, batch_size, nb_epoch, neurons)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mean_squared_error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1086\u001b[0m           self._maybe_load_initial_epoch_from_ckpt(initial_epoch))\n\u001b[0;32m   1087\u001b[0m       \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1088\u001b[1;33m       \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1089\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1090\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1131\u001b[0m     \u001b[1;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1133\u001b[1;33m       \u001b[0mdata_iterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1134\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    420\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    680\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0melement_spec\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    681\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 682\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    683\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    684\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    703\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[1;32m--> 705\u001b[1;33m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    706\u001b[0m       \u001b[1;31m# Delete the resource when this object is deleted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    707\u001b[0m       self._resource_deleter = IteratorResourceDeleter(\n",
      "\u001b[1;32mc:\\program files\\python38\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   2968\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2969\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2970\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   2971\u001b[0m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[0;32m   2972\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Start Time\n",
    "start_time = datetime.now().replace(microsecond=0)\n",
    "\n",
    "# run a repeated experiment\n",
    "def experiment_epochs(repeats, epochs):\n",
    "    #Get the values\n",
    "    grouped_values = grouped_mf_data.values\n",
    "\n",
    "    #Conver to linear series\n",
    "    linear_values = difference(grouped_values,1)\n",
    "\n",
    "    #transform to supervised learning\n",
    "    supervised_data = timeseries_to_supervised(linear_values,1)\n",
    "    supervised_values = supervised_data.values\n",
    "\n",
    "    #split into train and test data\n",
    "    train_data, test_data = get_train_test_data(supervised_values,70)\n",
    "   \n",
    "    # transform the scale of the data\n",
    "    scaler, train_scaled, test_scaled = scale(train_data, test_data)\n",
    "    \n",
    "    # run experiment\n",
    "    error_scores = list()\n",
    "    for r in range(repeats):\n",
    "        # fit the model\n",
    "        batch_size = 1\n",
    "        lstm_model = fit_lstm(train_scaled, batch_size, epochs, 1)\n",
    "        \n",
    "        # forecast the entire training dataset to build up state for forecasting\n",
    "        train_reshaped = train_scaled[:, 0].reshape(len(train_scaled), 1, 1)\n",
    "        lstm_model.predict(train_reshaped, batch_size=batch_size)\n",
    "        \n",
    "        # forecast test dataset\n",
    "        predictions = list()\n",
    "        for i in range(len(test_scaled)):\n",
    "            # make one-step forecast\n",
    "            X = test_scaled[i, 0:-1]\n",
    "            \n",
    "            yhat = forecast_lstm(lstm_model, 1, X)\n",
    "            \n",
    "            # invert scaling\n",
    "            yhat = invert_scale(scaler, X, yhat)\n",
    "            \n",
    "            # invert differencing\n",
    "            yhat = inverse_difference(grouped_values, yhat, len(test_scaled)+1-i)\n",
    "            \n",
    "            # store forecast\n",
    "            predictions.append(yhat)\n",
    "            \n",
    "        # report performance\n",
    "        seperator_value = get_percentage(70,len(grouped_values))\n",
    "        y_true = grouped_values[seperator_value:]\n",
    "        rmse = sqrt(mean_squared_error(y_true, predictions))\n",
    "        print('%d) Test RMSE: %.3f' % (r+1, rmse))\n",
    "        error_scores.append(rmse)\n",
    "    return error_scores\n",
    "\n",
    "# run a repeated experiment on neurons\n",
    "def experiment_neurons(repeats, epoch, neuron):\n",
    "    #Get the values\n",
    "    grouped_values = grouped_mf_data.values\n",
    "\n",
    "    #Conver to linear series\n",
    "    linear_values = difference(grouped_values,1)\n",
    "\n",
    "    #transform to supervised learning\n",
    "    supervised_data = timeseries_to_supervised(linear_values,1)\n",
    "    supervised_values = supervised_data.values\n",
    "\n",
    "    #split into train and test data\n",
    "    train_data, test_data = get_train_test_data(supervised_values,70)\n",
    "   \n",
    "    # transform the scale of the data\n",
    "    scaler, train_scaled, test_scaled = scale(train_data, test_data)\n",
    "    \n",
    "    # run experiment\n",
    "    error_scores = list()\n",
    "    for r in range(repeats):\n",
    "        # fit the model\n",
    "        batch_size = 1\n",
    "        lstm_model = fit_lstm(train_scaled, batch_size, epoch, neuron)\n",
    "        \n",
    "        # forecast the entire training dataset to build up state for forecasting\n",
    "        train_reshaped = train_scaled[:, 0].reshape(len(train_scaled), 1, 1)\n",
    "        lstm_model.predict(train_reshaped, batch_size=batch_size)\n",
    "        \n",
    "        # forecast test dataset\n",
    "        predictions = list()\n",
    "        for i in range(len(test_scaled)):\n",
    "            # make one-step forecast\n",
    "            X = test_scaled[i, 0:-1]\n",
    "            \n",
    "            yhat = forecast_lstm(lstm_model, 1, X)\n",
    "            \n",
    "            # invert scaling\n",
    "            yhat = invert_scale(scaler, X, yhat)\n",
    "            \n",
    "            # invert differencing\n",
    "            yhat = inverse_difference(grouped_values, yhat, len(test_scaled)+1-i)\n",
    "            \n",
    "            # store forecast\n",
    "            predictions.append(yhat)\n",
    "            \n",
    "        # report performance\n",
    "        seperator_value = get_percentage(70,len(grouped_values))\n",
    "        y_true = grouped_values[seperator_value:]\n",
    "        rmse = sqrt(mean_squared_error(y_true, predictions))\n",
    "        print('%d) Test RMSE: %.3f' % (r+1, rmse))\n",
    "        error_scores.append(rmse)\n",
    "    return error_scores\n",
    "\n",
    "# experiment\n",
    "repeats = 4\n",
    "\n",
    "results = pd.DataFrame()\n",
    "# vary training epochs\n",
    "#Ignore 6000 values as it is applicable for datasets>2000\n",
    "# epochs = [500, 1000, 2000, 4000, 6000]\n",
    "epochs = [500, 1000, 2000, 4000]\n",
    "# epochs = [500]\n",
    "for e in epochs:\n",
    "    results[str(e)] = experiment(repeats, e)\n",
    "# summarize results\n",
    "print(results.describe())\n",
    "# save boxplot\n",
    "results.boxplot()\n",
    "\n",
    "#Find the best fitted epoch from the previous step\n",
    "list_result = results.values.tolist()\n",
    "#The array shape transform => [[ep1re1,ep2re1,...],[ep1re2,ep2re2,...]] ==>  [[ep1re1,ep1re2,...],[ep2re1,ep2re2,...]]\n",
    "list_result_transposed = np.array(list_result).T.tolist()\n",
    "\n",
    "selected_index = 0\n",
    "#Getting the mean\n",
    "min_rse = pd.Series(list_result_transposed[0]).mean()\n",
    "\n",
    "rse_mean_list = []\n",
    "rse_mean_list.append(min_rse)\n",
    "\n",
    "for index in range(1,repeats):\n",
    "    current_min_rse = pd.Series(list_result_transposed[index]).mean()\n",
    "    if(current_min_rse < min_rse):\n",
    "        selected_index = index\n",
    "        min_rse = current_min_rse\n",
    "        \n",
    "\n",
    "best_fitted_epoch = epochs[selected_index]\n",
    "\n",
    "print(best_fitted_epoch)\n",
    "\n",
    "#Get neurons\n",
    "results = pd.DataFrame()\n",
    "# vary training epochs\n",
    "neurons = [1,2,3,4,5]\n",
    "for n in neurons:\n",
    "    column_name = \"Epoch - \" + str(best_fitted_epoch) + \" Neuron -\" + str(n)\n",
    "    results[column_name] = experiment(repeats, best_fitted_epoch, n)\n",
    "# summarize results\n",
    "print(results.describe())\n",
    "# save boxplot\n",
    "results.boxplot()\n",
    "\n",
    "#Find the best fitted neurons from the previous step\n",
    "list_result = results.values.tolist()\n",
    "#The array shape transform => [[ep1re1,ep2re1,...],[ep1re2,ep2re2,...]] ==>  [[ep1re1,ep1re2,...],[ep2re1,ep2re2,...]]\n",
    "list_result_transposed = np.array(list_result).T.tolist()\n",
    "\n",
    "selected_index = 0\n",
    "#Getting the mean\n",
    "min_rse = pd.Series(list_result_transposed[0]).mean()\n",
    "\n",
    "rse_mean_list = []\n",
    "rse_mean_list.append(min_rse)\n",
    "\n",
    "for index in range(1,repeats):\n",
    "    current_min_rse = pd.Series(list_result_transposed[index]).mean()\n",
    "    if(current_min_rse < min_rse):\n",
    "        selected_index = index\n",
    "        min_rse = current_min_rse\n",
    "        \n",
    "\n",
    "best_fitted_neuron = neurons[selected_index]\n",
    "\n",
    "print(best_fitted_neuron)\n",
    "\n",
    "#End Time\n",
    "end_time = datetime.now().replace(microsecond=0)\n",
    "\n",
    "print(\"Time Taken -\",end=\" \")\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the final training on the best fitted model\n",
    "\n",
    "1. We get the best fit epoch and neurons from the above observation\n",
    "2. We run a final training on the model with the best fit parameters\n",
    "3. We repeat the testing for 5 times.\n",
    "4. The prediction with the least RMSE is considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start Time\n",
    "start_time = datetime.now().replace(microsecond=0)\n",
    "\n",
    "# run a repeated experiment on neurons\n",
    "def final_training(epoch, neuron):\n",
    "    #Get the values\n",
    "    grouped_values = grouped_mf_data.values\n",
    "\n",
    "    #Conver to linear series\n",
    "    linear_values = difference(grouped_values,1)\n",
    "\n",
    "    #transform to supervised learning\n",
    "    supervised_data = timeseries_to_supervised(linear_values,1)\n",
    "    supervised_values = supervised_data.values\n",
    "\n",
    "    #split into train and test data\n",
    "    train_data, test_data = get_train_test_data(supervised_values,70)\n",
    "   \n",
    "    # transform the scale of the data\n",
    "    scaler, train_scaled, test_scaled = scale(train_data, test_data)\n",
    "    \n",
    "     # fit the model\n",
    "    batch_size = 1\n",
    "    lstm_model = fit_lstm(train_scaled, batch_size, epoch, neuron)\n",
    "\n",
    "    # forecast the entire training dataset to build up state for forecasting\n",
    "    train_reshaped = train_scaled[:, 0].reshape(len(train_scaled), 1, 1)\n",
    "    lstm_model.predict(train_reshaped, batch_size=batch_size)\n",
    "    \n",
    "#End Time\n",
    "end_time = datetime.now().replace(microsecond=0)\n",
    "\n",
    "print(\"Time Taken for Final Training -\",end=\" \")\n",
    "print(end_time - start_time)\n",
    "\n",
    "final_training(best_fitted_epoch, best_fitted_neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.9667169 ],\n",
       "       [-0.919257  ],\n",
       "       [-0.9818325 ],\n",
       "       [-0.9397556 ],\n",
       "       [-0.94729865],\n",
       "       [-0.939132  ],\n",
       "       [-0.92579424],\n",
       "       [-0.91148096],\n",
       "       [-0.90116763],\n",
       "       [-0.89132077],\n",
       "       [-0.8777171 ],\n",
       "       [-0.8662755 ],\n",
       "       [-0.8568841 ],\n",
       "       [-0.8490454 ],\n",
       "       [-0.84383404],\n",
       "       [-0.83789194],\n",
       "       [-0.83704865],\n",
       "       [-0.7751929 ],\n",
       "       [-0.77414256],\n",
       "       [-0.8200257 ],\n",
       "       [-0.7387691 ],\n",
       "       [-0.7932936 ],\n",
       "       [-0.775455  ],\n",
       "       [-0.7736205 ],\n",
       "       [-0.78080493],\n",
       "       [-0.7717776 ],\n",
       "       [-0.7957064 ],\n",
       "       [-0.797017  ],\n",
       "       [-0.80639255],\n",
       "       [-0.8112262 ],\n",
       "       [-0.8108473 ],\n",
       "       [-0.814378  ],\n",
       "       [-0.81775796],\n",
       "       [-0.8177829 ],\n",
       "       [-0.8175785 ],\n",
       "       [-0.817159  ],\n",
       "       [-0.8190041 ],\n",
       "       [-0.8119389 ],\n",
       "       [-0.8094236 ],\n",
       "       [-0.81333095],\n",
       "       [-0.7802831 ],\n",
       "       [-0.78608984],\n",
       "       [-0.8016495 ],\n",
       "       [-0.8072022 ],\n",
       "       [-0.5003441 ],\n",
       "       [-0.44570562],\n",
       "       [-0.5201782 ],\n",
       "       [-1.0331093 ],\n",
       "       [-1.1674066 ]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "lstm_model = fit_lstm(train_scaled, 1, 3000, 4)\n",
    "# forecast the entire training dataset to build up state for forecasting\n",
    "train_reshaped = train_scaled[:, 0].reshape(len(train_scaled), 1, 1)\n",
    "lstm_model.predict(train_reshaped, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month=1, Predicted=13770.113041, Expected=13935.640000\n",
      "Month=2, Predicted=13646.241770, Expected=14338.680000\n",
      "Month=3, Predicted=14246.361662, Expected=14407.180000\n",
      "Month=4, Predicted=14181.221261, Expected=14447.860000\n",
      "Month=5, Predicted=14204.575029, Expected=14444.750000\n",
      "Month=6, Predicted=14084.114858, Expected=14903.770000\n",
      "Month=7, Predicted=14737.391112, Expected=14903.770000\n",
      "Month=8, Predicted=14551.405835, Expected=14903.770000\n",
      "Month=9, Predicted=14474.663111, Expected=14859.560000\n",
      "Month=10, Predicted=14261.241452, Expected=14859.650000\n",
      "Month=11, Predicted=14143.717889, Expected=14819.300000\n",
      "Month=12, Predicted=13983.298178, Expected=14793.350000\n",
      "Month=13, Predicted=13878.471828, Expected=14786.760000\n",
      "Month=14, Predicted=13824.107976, Expected=14786.760000\n",
      "Month=15, Predicted=13792.476202, Expected=14786.760000\n",
      "Month=16, Predicted=13768.585212, Expected=14829.250000\n",
      "Month=17, Predicted=13809.840583, Expected=14848.780000\n",
      "Month=18, Predicted=13819.308431, Expected=14903.250000\n",
      "Month=19, Predicted=13880.088712, Expected=14923.360000\n",
      "Month=20, Predicted=13891.417731, Expected=14886.400000\n",
      "Month=21, Predicted=13824.579506, Expected=14886.400000\n"
     ]
    }
   ],
   "source": [
    "# walk-forward validation on the test data\n",
    "predictions = list()\n",
    "for i in range(len(test_scaled)):\n",
    "    # make one-step forecast\n",
    "    X, y = test_scaled[i, 0:-1], test_scaled[i, -1]\n",
    "    yhat = forecast_lstm(lstm_model, 1, X)\n",
    "    # invert scaling\n",
    "    yhat = invert_scale(scaler, X, yhat)\n",
    "    # invert differencing\n",
    "    yhat = inverse_difference(grouped_values, yhat, len(test_scaled)+1-i)\n",
    "    # store forecast\n",
    "    predictions.append(yhat)\n",
    "    expected = grouped_values[len(train_data) + i + 1]\n",
    "    print('Month=%d, Predicted=%f, Expected=%f' % (i+1, yhat, expected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Prediction for the upcoming days\n",
    "\n",
    "The following method uses -\n",
    "\n",
    "1. The # of days is based on the size of the sample.\n",
    "2. based on the size, the last test data is taken as X (time-step t-1).\n",
    "3. The data for next number of days is predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the number of days based on sample size\n",
    "def calculate_no_of_prediction_days(sample_size):\n",
    "    if(sample_size <= 1000):\n",
    "        return 3\n",
    "    elif (sample_size > 1000 and sample_size<=2000):\n",
    "        return 5\n",
    "    else:\n",
    "        return 7\n",
    "\n",
    "X_predict = test_scaled[-1, 0:-1]\n",
    "\n",
    "no_of_days = calculate_no_of_prediction_days(len(grouped_data_values))\n",
    "\n",
    "forecast_lstm_multi_step(lstm_model,no_of_days,X_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Analysis\n",
    "\n",
    "A line plot of the test data (blue) vs the predicted values (orange) is also created, providing context for the model skill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [22, 21]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-78aad005531b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrouped_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mseperator_value\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mrmse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test RMSE: %.3f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mrmse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\metrics\\_regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m     \"\"\"\n\u001b[1;32m--> 255\u001b[1;33m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0m\u001b[0;32m    256\u001b[0m         y_true, y_pred, multioutput)\n\u001b[0;32m    257\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\metrics\\_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \"\"\"\n\u001b[1;32m---> 84\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m     \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[0;32m    256\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [22, 21]"
     ]
    }
   ],
   "source": [
    "# report performance\n",
    "seperator_value = get_percentage(70,len(grouped_values))\n",
    "y_true = grouped_values[seperator_value:]\n",
    "\n",
    "rmse = sqrt(mean_squared_error(y_true, predictions))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "\n",
    "# line plot of observed vs predicted\n",
    "pyplot.plot(y_true)\n",
    "pyplot.plot(predictions)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the final Comparision Graph (To be sent in mail, everyday)\n",
    "\n",
    "1. Create a Dataset with next three days data\n",
    "2. Add the next three days predicted value in the dataset\n",
    "3. Create the first line - Original Values (Red/Yellow/Green)\n",
    "4. Create the second line - Predicted Values (#db3e32/#d0db32/#32db6a)\n",
    "5. Currently there would be no intersection points between the two lines. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
